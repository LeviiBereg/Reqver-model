{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as tfh\n",
    "from bert.tokenization import FullTokenizer\n",
    "from gensim.models import KeyedVectors as word2vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"java\" #\"python\"\n",
    "DATA_PATH = \"../../Data/code2desc\"\n",
    "DATA_FOLDER = f\"{LANGUAGE}/short\"\n",
    "TRAIN_FILE  = f\"{LANGUAGE}_train_0.jsonl\"\n",
    "TEST_FILE   = f\"{LANGUAGE}_test_0.jsonl\"\n",
    "VALID_FILE  = f\"{LANGUAGE}_valid_0.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire tokenized source code and plain docstrings.\n",
    "# BERT uses its own 'FullTokenizer' for inputs.\n",
    "use_cols = [\"code_tokens\", \"docstring\"]\n",
    "train_df = pd.read_json(f\"{DATA_PATH}/{DATA_FOLDER}/{TRAIN_FILE}\", lines=True)[use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_tokens  30000 non-null  object\n",
      " 1   docstring    30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[protected, final, void, bindIndexed, (, Confi...</td>\n",
       "      <td>Bind indexed elements to the supplied collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[public, void, setServletRegistrationBeans, (,...</td>\n",
       "      <td>Set {@link ServletRegistrationBean}s that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[public, void, addServletRegistrationBeans, (,...</td>\n",
       "      <td>Add {@link ServletRegistrationBean}s for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[public, void, setServletNames, (, Collection,...</td>\n",
       "      <td>Set servlet names that the filter will be regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[public, void, addServletNames, (, String, ......</td>\n",
       "      <td>Add servlet names for the filter.\\n@param serv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         code_tokens  \\\n",
       "0  [protected, final, void, bindIndexed, (, Confi...   \n",
       "1  [public, void, setServletRegistrationBeans, (,...   \n",
       "2  [public, void, addServletRegistrationBeans, (,...   \n",
       "3  [public, void, setServletNames, (, Collection,...   \n",
       "4  [public, void, addServletNames, (, String, ......   \n",
       "\n",
       "                                           docstring  \n",
       "0  Bind indexed elements to the supplied collecti...  \n",
       "1  Set {@link ServletRegistrationBean}s that the ...  \n",
       "2  Add {@link ServletRegistrationBean}s for the f...  \n",
       "3  Set servlet names that the filter will be regi...  \n",
       "4  Add servlet names for the filter.\\n@param serv...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at <a href=\"https://github.com/tensorflow/models/tree/master/official/nlp/bert\">tensorflow/models/official/nlp/bert</a>. It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads.\n",
    "\n",
    "This model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. For training, random input masking has been applied independently to word pieces (as in the original BERT paper).\n",
    "\n",
    "All parameters in the module are trainable, and fine-tuning all parameters is the recommended practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = tfh.KerasLayer(model_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FOLDER = \"source-code-embeddings\"\n",
    "TOKEN_EMBEDDINGS  = \"token_vecs.txt\"\n",
    "TARGET_EMBEDDINGS = \"target_vecs.txt\"\n",
    "\n",
    "vectors_text_path = f'{EMBEDDINGS_FOLDER}/{TOKEN_EMBEDDINGS}'\n",
    "model = word2vec.load_word2vec_format(vectors_text_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    '''Performs cleaning of text of unwanted symbols, \n",
    "    excessive spaces and transfers to lower-case\n",
    "    '''\n",
    "#     punct_regxp = re.compile(f'([{string.punctuation}])')\n",
    "#     text = re.sub(punct_regxp, r\" \\1 \", text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    \n",
    "    text = ''.join(character for character in text if character in string.printable)\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.docstring = train_df.docstring.apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_input(text, max_seq_length):\n",
    "\n",
    "    tokenized_text = [[\"[CLS]\"] + tokenizer.tokenize(seq)[:max_seq_length-2] + [\"[SEP]\"] for seq in text]\n",
    "    input_ids   = [tokenizer.convert_tokens_to_ids(tokens_seq) for tokens_seq in tokenized_text]\n",
    "    input_mask  = [[1] * len(input_seq) for input_seq in input_ids]\n",
    "    segment_ids = [[0] * max_seq_length for _ in range(len(input_ids))]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_seq_length, padding='post')\n",
    "    input_mask = tf.keras.preprocessing.sequence.pad_sequences(input_mask, maxlen=max_seq_length, padding='post')\n",
    "    segment_ids = tf.keras.preprocessing.sequence.pad_sequences(segment_ids, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    return input_ids, input_mask, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_max_seq_length = 256\n",
    "desc_word_ids, desc_input_mask, desc_segment_ids = generate_desc_input(train_df.docstring, desc_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sc_input(sc_inputs, emb_model, max_seq_length):\n",
    "    \n",
    "    def word_to_index(word):\n",
    "        word_val = emb_model.vocab.get(word, None)\n",
    "        word_index = word_val.index if word_val else None\n",
    "        return word_index\n",
    "    \n",
    "    input_ids = [[word_to_index(word) for word in sc_input[:max_seq_length] if word in emb_model.vocab.keys()] \\\n",
    "             for sc_input in sc_inputs]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
    "                                                        dtype='int32', \n",
    "                                                        maxlen=max_seq_length, \n",
    "                                                        padding='post')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_max_seq_length = 256\n",
    "sc_ids = generate_sc_input(train_df.code_tokens, model, sc_max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "\n",
    "# input_word_ids = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                        dtype=tf.int32,\n",
    "#                                        name=\"desc_input_word_ids\")\n",
    "# input_mask  = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                    dtype=tf.int32,\n",
    "#                                    name=\"desc_input_mask\")\n",
    "# segment_ids = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                     dtype=tf.int32,\n",
    "#                                     name=\"desc_segment_ids\")\n",
    "\n",
    "desc_dense = tf.keras.layers.Dense(dense_units, activation='sigmoid', name=\"desc_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def desc_propagate(input_word_ids, input_mask, segment_ids):\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    desc_output = desc_dense(pooled_output)\n",
    "    return desc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_get_trainable_parameters():\n",
    "    tr_vars = desc_dense.trainable_variables\n",
    "    return tr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_lstm_units = 256\n",
    "sc_model = 'convolutional' # 'lstm'\n",
    "conv_kernel_sizes = [2,3,5]\n",
    "conv_n_filters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "# input_sc_ids = tf.keras.layers.Input(shape=(sc_max_seq_length,), \n",
    "#                                        dtype=tf.int32,\n",
    "#                                        name=\"sc_input_ids\")\n",
    "\n",
    "sc_embedding = tf.keras.layers.Embedding(len(model.vocab),\n",
    "                                         model.vector_size, \n",
    "                                         weights=[model.vectors],\n",
    "                                         mask_zero=True,\n",
    "                                         trainable=True,\n",
    "                                         name=\"sc_embedding\") # (vocab_size, vec_size) (1294891, 128)\n",
    "\n",
    "if sc_model == 'convolutional':\n",
    "    sc_convs = []\n",
    "    sc_max_pools = []\n",
    "    for kernel_size in conv_kernel_sizes:\n",
    "        sc_convs.append(tf.keras.layers.Conv1D(conv_n_filters, kernel_size, activation='relu', name=f'conv_{kernel_size}'))\n",
    "        sc_max_pools.append(tf.keras.layers.MaxPooling1D(sc_max_seq_length - kernel_size + 1, 1, name=f'max_pool_{kernel_size}'))\n",
    "elif sc_model == 'lstm':\n",
    "    sc_lstm = tf.keras.layers.LSTM(sc_lstm_units, name=\"sc_lstm\")\n",
    "\n",
    "sc_dense = tf.keras.layers.Dense(dense_units, activation='sigmoid', name=\"desc_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def sc_propagate(input_sc_ids):\n",
    "    sc_embedded_input = sc_embedding(input_sc_ids) # (batch_size, sc_max_seq_length, emb_vec_size)\n",
    "    if sc_model == 'convolutional':\n",
    "        conv_outputs = []\n",
    "        for sc_conv, sc_max_pool in zip(sc_convs, sc_max_pools):\n",
    "            sc_conv_out = sc_conv(sc_embedded_input) \n",
    "            conv_outputs.append(sc_max_pool(sc_conv_out))\n",
    "        sc_output = tf.concat(conv_outputs, 2) # (batch_size, 1, n_convs * conv_n_filters)\n",
    "        sc_output = tf.reshape(sc_output, [-1, len(conv_kernel_sizes) * conv_n_filters]) # (batch_size, n_convs * conv_n_filters)\n",
    "    elif sc_model == 'lstm':\n",
    "        sc_output = sc_lstm(sc_embedded_input) #  (batch_size, sc_lstm_units)\n",
    "    sc_output = sc_dense(sc_output) # (batch_size, dense_units)\n",
    "    return sc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_get_trainable_parameters():\n",
    "    tr_vars = sc_dense.trainable_variables + sc_embedding.trainable_variables\n",
    "    if sc_model == 'convolutional':\n",
    "        for sc_conv in sc_convs:\n",
    "            tr_vars += sc_conv.trainable_variables\n",
    "    elif sc_model == 'lstm':\n",
    "        tr_vars += sc_lstm.trainable_variables\n",
    "    return tr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branches junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def compute_similarity(desc_output, sc_output):\n",
    "    norm_desc = tf.nn.l2_normalize(desc_output, axis=1, name=\"desc_output_norm\")        \n",
    "    norm_sc   = tf.nn.l2_normalize(sc_output, axis=1, name=\"sc_output_norm\")\n",
    "    cos_similarity = tf.reduce_sum(tf.multiply(norm_desc, norm_sc, name=\"b_outputs_dot\"), \n",
    "                                   axis=1, \n",
    "                                   name=\"cos_similarity\")\n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Redundant #####\n",
    "\n",
    "# inputs = [input_word_ids, input_mask, segment_ids, input_sc_ids]\n",
    "# outputs = cos_similarity\n",
    "\n",
    "# sim_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = tf.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# sim_model.compile(loss=loss_func, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "\n",
    "    loss = loss_func(y_true, y_pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def negative_sampling(desc_output, sc_output):\n",
    "    neg_probs = tf.linalg.set_diag(tf.fill([batch_size, batch_size], 0.5),[0]*batch_size)\n",
    "    neg_ids = tf.random.categorical(neg_probs, n_negatives)\n",
    "\n",
    "    neg_desc = tf.reshape(tf.gather(desc_output, neg_ids), [-1, dense_units])\n",
    "    neg_sc = tf.reshape(tf.gather(sc_output, [[i]*n_negatives for i in range(batch_size)]), [-1, dense_units])\n",
    "\n",
    "    desc_output = tf.concat([desc_output,neg_desc], axis=0)\n",
    "    sc_output = tf.concat([sc_output,neg_sc], axis=0)\n",
    "    \n",
    "    return desc_output, sc_output\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_word_ids, input_mask, segment_ids, input_sc_ids, batch_size, n_negatives):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "        sc_output = sc_propagate(input_sc_ids)\n",
    "        \n",
    "        desc_output, sc_output = negative_sampling(desc_output, sc_output)\n",
    "        cos_similarity = compute_similarity(desc_output, sc_output)\n",
    "        labels = [1] * batch_size + [0] * (batch_size * n_negatives)\n",
    "        loss = loss_function(labels, cos_similarity)\n",
    "    # Adjust the parameters of the model using the computed gradients\n",
    "    variables = desc_get_trainable_parameters() + sc_get_trainable_parameters()\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def validation_step(input_word_ids, input_mask, segment_ids, input_sc_ids, batch_size, n_negatives):\n",
    "\n",
    "    desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "    sc_output = sc_propagate(input_sc_ids)\n",
    "\n",
    "    desc_output, sc_output = negative_sampling(desc_output, sc_output)\n",
    "    cos_similarity = compute_similarity(desc_output, sc_output)\n",
    "    labels = [1] * batch_size + [0] * (batch_size * n_negatives)\n",
    "    loss = loss_function(labels, cos_similarity)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = train_test_split(desc_word_ids, desc_input_mask, desc_segment_ids, sc_ids)\n",
    "train_desc_word_ids, test_desc_word_ids = splitted_data[:2]\n",
    "train_desc_input_mask, test_desc_input_mask = splitted_data[2:4]\n",
    "train_desc_segment_ids, test_desc_segment_ids = splitted_data[4:6]\n",
    "train_sc_ids, test_sc_ids = splitted_data[6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_samples = len(train_desc_word_ids)\n",
    "valid_samples = len(test_desc_word_ids)\n",
    "train_steps_per_epoch = train_samples // batch_size\n",
    "valid_steps_per_epoch = valid_samples // batch_size\n",
    "epochs = 5\n",
    "n_negatives = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_desc_word_ids, train_desc_input_mask, train_desc_segment_ids, train_sc_ids)).shuffle(len(train_desc_word_ids), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((test_desc_word_ids, test_desc_input_mask, test_desc_segment_ids, test_sc_ids)).shuffle(len(test_desc_word_ids), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.415342092514038\n",
      "Epoch 1 Batch 17 Loss 0.8460800051689148\n",
      "Epoch 1 Batch 34 Loss 0.3809548020362854\n",
      "Epoch 1 Batch 51 Loss 0.23988837003707886\n",
      "Epoch 1 Batch 68 Loss 0.1757964789867401\n",
      "Epoch 1 Batch 85 Loss 0.15545348823070526\n",
      "Epoch 1 Train loss 0.5574725270271301 Val loss 0.42419907450675964 Time spent 1250.1830506324768 sec\n",
      "Epoch 2 Batch 0 Loss 0.14253267645835876\n",
      "Epoch 2 Batch 17 Loss 0.1228240355849266\n",
      "Epoch 2 Batch 34 Loss 0.10881292074918747\n",
      "Epoch 2 Batch 51 Loss 0.10960756987333298\n",
      "Epoch 2 Batch 68 Loss 0.10081747174263\n",
      "Epoch 2 Batch 85 Loss 0.10182619839906693\n",
      "Epoch 2 Train loss 0.11126609146595001 Val loss 0.2812860310077667 Time spent 1240.616108417511 sec\n",
      "Epoch 3 Batch 0 Loss 0.09414134174585342\n",
      "Epoch 3 Batch 17 Loss 0.08793734014034271\n",
      "Epoch 3 Batch 34 Loss 0.08356796205043793\n",
      "Epoch 3 Batch 51 Loss 0.08182909339666367\n",
      "Epoch 3 Batch 68 Loss 0.0843687430024147\n",
      "Epoch 3 Batch 85 Loss 0.08056549727916718\n",
      "Epoch 3 Train loss 0.08676941692829132 Val loss 0.2449016124010086 Time spent 1264.4980845451355 sec\n",
      "Epoch 4 Batch 0 Loss 0.07989107817411423\n",
      "Epoch 4 Batch 17 Loss 0.08016683161258698\n",
      "Epoch 4 Batch 34 Loss 0.07865515351295471\n",
      "Epoch 4 Batch 51 Loss 0.07822222262620926\n",
      "Epoch 4 Batch 68 Loss 0.07607194036245346\n",
      "Epoch 4 Batch 85 Loss 0.07439243048429489\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "batch_loss_frequency = train_steps_per_epoch // 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # Perform training steps on the training data batches\n",
    "    for (batch, (bdesc_word_ids, bdesc_input_mask, bdesc_segment_ids, bsc_ids)) in enumerate(train_data.take(train_steps_per_epoch)):\n",
    "        batch_loss = train_step(bdesc_word_ids, bdesc_input_mask, bdesc_segment_ids, bsc_ids, batch_size, n_negatives)\n",
    "        total_loss += batch_loss\n",
    "        if batch % batch_loss_frequency == 0:\n",
    "            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy()}\")\n",
    "            \n",
    "    for (batch, (vbdesc_word_ids, vbdesc_input_mask, vbdesc_segment_ids, vbsc_ids)) in enumerate(train_data.take(train_steps_per_epoch)):\n",
    "        vbatch_loss = validation_step(vbdesc_word_ids, vbdesc_input_mask, vbdesc_segment_ids, vbsc_ids, batch_size, n_negatives)\n",
    "        total_val_loss += vbatch_loss\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} Train loss {total_loss/train_steps_per_epoch} Val loss {total_val_loss/valid_steps_per_epoch} Time spent {time.time()-start} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_word_ids, input_mask, segment_ids, input_sc_ids):\n",
    "    desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "    sc_output = sc_propagate(input_sc_ids)\n",
    "\n",
    "    desc_output, sc_output = negative_sampling(desc_output, sc_output)\n",
    "\n",
    "    neg_probs = tf.linalg.set_diag(tf.fill([batch_size, batch_size], 0.5),[0]*batch_size)\n",
    "    neg_ids = tf.random.categorical(neg_probs, n_negatives)\n",
    "\n",
    "    neg_desc = tf.reshape(tf.gather(desc_output, neg_ids), [-1, dense_units])\n",
    "    neg_sc = tf.reshape(tf.gather(sc_output, [[i]*n_negatives for i in range(batch_size)]), [-1, dense_units])\n",
    "    \n",
    "    desc_output = tf.concat([desc_output,neg_desc], axis=0)\n",
    "    sc_output = tf.concat([sc_output,neg_sc], axis=0)\n",
    "    \n",
    "    cos_similarity = compute_similarity(desc_output, sc_output)\n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids, input_mask, segment_ids, input_sc_ids = list(train_data.take(1).as_numpy_iterator())[0]\n",
    "evaluate(input_word_ids, input_mask, segment_ids, input_sc_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
