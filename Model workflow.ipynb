{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as tfh\n",
    "from bert.tokenization import FullTokenizer\n",
    "from gensim.models import KeyedVectors as word2vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"java\" #\"python\"\n",
    "DATA_PATH = \"../../Data/code2desc\"\n",
    "DATA_FOLDER = f\"{LANGUAGE}/short\"\n",
    "TRAIN_FILE  = f\"{LANGUAGE}_train_0.jsonl\"\n",
    "TEST_FILE   = f\"{LANGUAGE}_test_0.jsonl\"\n",
    "VALID_FILE  = f\"{LANGUAGE}_valid_0.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire tokenized source code and plain docstrings.\n",
    "# BERT uses its own 'FullTokenizer' for inputs.\n",
    "use_cols = [\"code_tokens\", \"docstring\"]\n",
    "train_df = pd.read_json(f\"{DATA_PATH}/{DATA_FOLDER}/{TRAIN_FILE}\", lines=True)[use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_tokens  30000 non-null  object\n",
      " 1   docstring    30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[protected, final, void, bindIndexed, (, Confi...</td>\n",
       "      <td>Bind indexed elements to the supplied collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[public, void, setServletRegistrationBeans, (,...</td>\n",
       "      <td>Set {@link ServletRegistrationBean}s that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[public, void, addServletRegistrationBeans, (,...</td>\n",
       "      <td>Add {@link ServletRegistrationBean}s for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[public, void, setServletNames, (, Collection,...</td>\n",
       "      <td>Set servlet names that the filter will be regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[public, void, addServletNames, (, String, ......</td>\n",
       "      <td>Add servlet names for the filter.\\n@param serv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         code_tokens  \\\n",
       "0  [protected, final, void, bindIndexed, (, Confi...   \n",
       "1  [public, void, setServletRegistrationBeans, (,...   \n",
       "2  [public, void, addServletRegistrationBeans, (,...   \n",
       "3  [public, void, setServletNames, (, Collection,...   \n",
       "4  [public, void, addServletNames, (, String, ......   \n",
       "\n",
       "                                           docstring  \n",
       "0  Bind indexed elements to the supplied collecti...  \n",
       "1  Set {@link ServletRegistrationBean}s that the ...  \n",
       "2  Add {@link ServletRegistrationBean}s for the f...  \n",
       "3  Set servlet names that the filter will be regi...  \n",
       "4  Add servlet names for the filter.\\n@param serv...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at <a href=\"https://github.com/tensorflow/models/tree/master/official/nlp/bert\">tensorflow/models/official/nlp/bert</a>. It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads.\n",
    "\n",
    "This model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. For training, random input masking has been applied independently to word pieces (as in the original BERT paper).\n",
    "\n",
    "All parameters in the module are trainable, and fine-tuning all parameters is the recommended practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = tfh.KerasLayer(model_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FOLDER = \"source-code-embeddings\"\n",
    "TOKEN_EMBEDDINGS  = \"token_vecs.txt\"\n",
    "TARGET_EMBEDDINGS = \"target_vecs.txt\"\n",
    "\n",
    "vectors_text_path = f'{EMBEDDINGS_FOLDER}/{TOKEN_EMBEDDINGS}'\n",
    "model = word2vec.load_word2vec_format(vectors_text_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    '''Performs cleaning of text of unwanted symbols, \n",
    "    excessive spaces and transfers to lower-case\n",
    "    '''\n",
    "#     punct_regxp = re.compile(f'([{string.punctuation}])')\n",
    "#     text = re.sub(punct_regxp, r\" \\1 \", text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    \n",
    "    text = ''.join(character for character in text if character in string.printable)\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.docstring = train_df.docstring.apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_input(text, max_seq_length):\n",
    "\n",
    "    tokenized_text = [[\"[CLS]\"] + tokenizer.tokenize(seq)[:max_seq_length-2] + [\"[SEP]\"] for seq in text]\n",
    "    input_ids   = [tokenizer.convert_tokens_to_ids(tokens_seq) for tokens_seq in tokenized_text]\n",
    "    input_mask  = [[1] * len(input_seq) for input_seq in input_ids]\n",
    "    segment_ids = [[0] * max_seq_length for _ in range(len(input_ids))]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_seq_length, padding='post')\n",
    "    input_mask = tf.keras.preprocessing.sequence.pad_sequences(input_mask, maxlen=max_seq_length, padding='post')\n",
    "    segment_ids = tf.keras.preprocessing.sequence.pad_sequences(segment_ids, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    return input_ids, input_mask, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_max_seq_length = 256\n",
    "desc_word_ids, desc_input_mask, desc_segment_ids = generate_desc_input(train_df.docstring, desc_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sc_input(sc_inputs, emb_model, max_seq_length):\n",
    "    \n",
    "    def word_to_index(word):\n",
    "        word_val = emb_model.vocab.get(word, None)\n",
    "        word_index = word_val.index if word_val else None\n",
    "        return word_index\n",
    "    \n",
    "    input_ids = [[word_to_index(word) for word in sc_input[:max_seq_length] if word in emb_model.vocab.keys()] \\\n",
    "             for sc_input in sc_inputs]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
    "                                                        dtype='int32', \n",
    "                                                        maxlen=max_seq_length, \n",
    "                                                        padding='post')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_max_seq_length = 256\n",
    "sc_ids = generate_sc_input(train_df.code_tokens, model, sc_max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_units = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "\n",
    "# input_word_ids = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                        dtype=tf.int32,\n",
    "#                                        name=\"desc_input_word_ids\")\n",
    "# input_mask  = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                    dtype=tf.int32,\n",
    "#                                    name=\"desc_input_mask\")\n",
    "# segment_ids = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                     dtype=tf.int32,\n",
    "#                                     name=\"desc_segment_ids\")\n",
    "\n",
    "desc_dense = tf.keras.layers.Dense(dense_units, name=\"desc_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "# desc_output = desc_dense(pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "# input_sc_ids = tf.keras.layers.Input(shape=(sc_max_seq_length,), \n",
    "#                                        dtype=tf.int32,\n",
    "#                                        name=\"sc_input_ids\")\n",
    "\n",
    "sc_embedding = tf.keras.layers.Embedding(len(model.vocab),\n",
    "                                         model.vector_size, \n",
    "                                         weights=[model.vectors],\n",
    "                                         name=\"sc_embedding\")\n",
    "\n",
    "sc_lstm = tf.keras.layers.LSTM(dense_units, name=\"sc_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc_embedded_input = sc_embedding(input_sc_ids)\n",
    "# sc_output = sc_lstm(sc_embedded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branches junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "# norm_desc = tf.nn.l2_normalize(desc_output, axis=0, name=\"desc_output_norm\")        \n",
    "# norm_sc   = tf.nn.l2_normalize(sc_output, axis=0, name=\"sc_output_norm\")\n",
    "# cos_similarity = tf.reduce_sum(tf.multiply(norm_desc, norm_sc, name=\"b_outputs_dot\"), \n",
    "#                                axis=1, \n",
    "#                                name=\"cos_similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Redundant #####\n",
    "\n",
    "# inputs = [input_word_ids, input_mask, segment_ids, input_sc_ids]\n",
    "# outputs = cos_similarity\n",
    "\n",
    "# sim_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = tf.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# sim_model.compile(loss=loss_func, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "desc_input_word_ids (InputLayer [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_input_mask (InputLayer)    [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_segment_ids (InputLayer)   [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sc_input_ids (InputLayer)       [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   desc_input_word_ids[0][0]        \n",
      "                                                                 desc_input_mask[0][0]            \n",
      "                                                                 desc_segment_ids[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sc_embedding (Embedding)        (None, 256, 128)     165746048   sc_input_ids[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "desc_dense (Dense)              (None, 128)          98432       keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sc_lstm (LSTM)                  (None, 128)          131584      sc_embedding[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_desc_output_norm/Sq [(None, 128)]        0           desc_dense[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sc_output_norm/Squa [(None, 128)]        0           sc_lstm[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_desc_output_norm/Su [(1, 128)]           0           tf_op_layer_desc_output_norm/Squa\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sc_output_norm/Sum  [(1, 128)]           0           tf_op_layer_sc_output_norm/Square\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_desc_output_norm/Ma [(1, 128)]           0           tf_op_layer_desc_output_norm/Sum[\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sc_output_norm/Maxi [(1, 128)]           0           tf_op_layer_sc_output_norm/Sum[0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_desc_output_norm/Rs [(1, 128)]           0           tf_op_layer_desc_output_norm/Maxi\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sc_output_norm/Rsqr [(1, 128)]           0           tf_op_layer_sc_output_norm/Maximu\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_desc_output_norm (T [(None, 128)]        0           desc_dense[0][0]                 \n",
      "                                                                 tf_op_layer_desc_output_norm/Rsqr\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sc_output_norm (Ten [(None, 128)]        0           sc_lstm[0][0]                    \n",
      "                                                                 tf_op_layer_sc_output_norm/Rsqrt[\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_b_outputs_dot (Tens [(None, 128)]        0           tf_op_layer_desc_output_norm[0][0\n",
      "                                                                 tf_op_layer_sc_output_norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_cos_similarity (Ten [(None,)]            0           tf_op_layer_b_outputs_dot[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 275,458,305\n",
      "Trainable params: 165,976,064\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sim_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "\n",
    "    loss = loss_func(y_true, y_pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_word_ids, input_mask, segment_ids, input_sc_ids, batch_size, n_negatives):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "        desc_output = desc_dense(pooled_output)\n",
    "        \n",
    "        sc_embedded_input = sc_embedding(input_sc_ids)\n",
    "        sc_output = sc_lstm(sc_embedded_input)\n",
    "        \n",
    "        neg_probs = tf.linalg.set_diag(tf.fill([batch_size, batch_size], 0.5),[0]*batch_size)\n",
    "        neg_ids = tf.random.categorical(neg_probs, n_negatives)\n",
    "        \n",
    "        neg_desc = tf.reshape(tf.gather(desc_output, neg_ids), [-1, dense_units])\n",
    "        neg_sc = tf.reshape(tf.gather(sc_output, [[i]*n_negatives for i in range(batch_size)]), [-1, dense_units])\n",
    "        \n",
    "        desc_output = tf.concat([desc_output,neg_desc], axis=0)\n",
    "        sc_output = tf.concat([sc_output,neg_sc], axis=0)\n",
    "        \n",
    "        norm_desc = tf.nn.l2_normalize(desc_output, axis=0, name=\"desc_output_norm\")        \n",
    "        norm_sc   = tf.nn.l2_normalize(sc_output, axis=0, name=\"sc_output_norm\")\n",
    "        cos_similarity = tf.reduce_sum(tf.multiply(norm_desc, norm_sc, name=\"b_outputs_dot\"), \n",
    "                                       axis=1, \n",
    "                                       name=\"cos_similarity\")\n",
    "        labels = [1] * batch_size + [0] * (batch_size * n_negatives)\n",
    "        loss = loss_function(labels, cos_similarity)\n",
    "    # Adjust the parameters of the model using the computed gradients\n",
    "    variables = desc_dense.trainable_variables   \\\n",
    "              + sc_embedding.trainable_variables \\\n",
    "              + sc_lstm.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = train_test_split(desc_word_ids, desc_input_mask, desc_segment_ids, sc_ids)\n",
    "train_desc_word_ids, test_desc_word_ids = splitted_data[:2]\n",
    "train_desc_input_mask, test_desc_input_mask = splitted_data[2:4]\n",
    "train_desc_segment_ids, test_desc_segment_ids = splitted_data[4:6]\n",
    "train_sc_ids, test_sc_ids = splitted_data[6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_samples = len(train_desc_word_ids)\n",
    "train_steps_per_epoch = train_samples//batch_size\n",
    "epochs = 10\n",
    "n_negatives = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_desc_word_ids, train_desc_input_mask, train_desc_segment_ids, train_sc_ids)).shuffle(len(train_desc_word_ids)).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.6798810958862305\n",
      "Epoch 1 Batch 9 Loss 0.2418292760848999\n",
      "Epoch 1 Batch 18 Loss 0.21766351163387299\n",
      "Epoch 1 Batch 27 Loss 0.21638987958431244\n",
      "Epoch 1 Batch 36 Loss 0.21137230098247528\n",
      "Epoch 1 Batch 45 Loss 0.20786988735198975\n",
      "Epoch 1 Batch 54 Loss 0.20658765733242035\n",
      "Epoch 1 Batch 63 Loss 0.20467731356620789\n",
      "Epoch 1 Batch 72 Loss 0.20442187786102295\n",
      "Epoch 1 Batch 81 Loss 0.20410247147083282\n",
      "Epoch 1 Batch 90 Loss 0.2029789239168167\n",
      "Epoch 1 Batch 99 Loss 0.20331209897994995\n",
      "Epoch 1 Batch 108 Loss 0.2026626467704773\n",
      "Epoch 1 Batch 117 Loss 0.20277513563632965\n",
      "Epoch 1 Batch 126 Loss 0.2026955783367157\n",
      "Epoch 1 Batch 135 Loss 0.20254820585250854\n",
      "Epoch 1 Batch 144 Loss 0.20230887830257416\n",
      "Epoch 1 Batch 153 Loss 0.20221416652202606\n",
      "Epoch 1 Batch 162 Loss 0.20130252838134766\n",
      "Epoch 1 Batch 171 Loss 0.20525065064430237\n",
      "Epoch 2 Batch 0 Loss 0.2011537253856659\n",
      "Epoch 2 Batch 9 Loss 0.20068900287151337\n",
      "Epoch 2 Batch 18 Loss 0.20030464231967926\n",
      "Epoch 2 Batch 27 Loss 0.19983221590518951\n",
      "Epoch 2 Batch 36 Loss 0.19938446581363678\n",
      "Epoch 2 Batch 45 Loss 0.19973161816596985\n",
      "Epoch 2 Batch 54 Loss 0.19959445297718048\n",
      "Epoch 2 Batch 63 Loss 0.1993212252855301\n",
      "Epoch 2 Batch 72 Loss 0.19922374188899994\n",
      "Epoch 2 Batch 81 Loss 0.1989600956439972\n",
      "Epoch 2 Batch 90 Loss 0.19819030165672302\n",
      "Epoch 2 Batch 99 Loss 0.19777411222457886\n",
      "Epoch 2 Batch 108 Loss 0.1976867914199829\n",
      "Epoch 2 Batch 117 Loss 0.1975090205669403\n",
      "Epoch 2 Batch 126 Loss 0.19727203249931335\n",
      "Epoch 2 Batch 135 Loss 0.19728140532970428\n",
      "Epoch 2 Batch 144 Loss 0.19717152416706085\n",
      "Epoch 2 Batch 153 Loss 0.19672991335391998\n",
      "Epoch 2 Batch 162 Loss 0.1969994604587555\n",
      "Epoch 2 Batch 171 Loss 0.19677291810512543\n",
      "Epoch 3 Batch 0 Loss 0.19684912264347076\n",
      "Epoch 3 Batch 9 Loss 0.19657868146896362\n",
      "Epoch 3 Batch 18 Loss 0.19657723605632782\n",
      "Epoch 3 Batch 27 Loss 0.19708964228630066\n",
      "Epoch 3 Batch 36 Loss 0.1965661346912384\n",
      "Epoch 3 Batch 45 Loss 0.1962946504354477\n",
      "Epoch 3 Batch 54 Loss 0.1962084323167801\n",
      "Epoch 3 Batch 63 Loss 0.1961650550365448\n",
      "Epoch 3 Batch 72 Loss 0.1961783617734909\n",
      "Epoch 3 Batch 81 Loss 0.20000877976417542\n",
      "Epoch 3 Batch 90 Loss 0.19609448313713074\n",
      "Epoch 3 Batch 99 Loss 0.19590182602405548\n",
      "Epoch 3 Batch 108 Loss 0.19687668979167938\n",
      "Epoch 3 Batch 117 Loss 0.19604283571243286\n",
      "Epoch 3 Batch 126 Loss 0.19573800265789032\n",
      "Epoch 3 Batch 135 Loss 0.19582021236419678\n",
      "Epoch 3 Batch 144 Loss 0.19574807584285736\n",
      "Epoch 3 Batch 153 Loss 0.1957816779613495\n",
      "Epoch 3 Batch 162 Loss 0.19547206163406372\n",
      "Epoch 3 Batch 171 Loss 0.19549287855625153\n",
      "Epoch 4 Batch 0 Loss 0.19537471234798431\n",
      "Epoch 4 Batch 9 Loss 0.19519874453544617\n",
      "Epoch 4 Batch 18 Loss 0.19574953615665436\n",
      "Epoch 4 Batch 27 Loss 0.1950443983078003\n",
      "Epoch 4 Batch 36 Loss 0.19488456845283508\n",
      "Epoch 4 Batch 45 Loss 0.1950109750032425\n",
      "Epoch 4 Batch 54 Loss 0.1947694569826126\n",
      "Epoch 4 Batch 63 Loss 0.1947106122970581\n",
      "Epoch 4 Batch 72 Loss 0.19486920535564423\n",
      "Epoch 4 Batch 81 Loss 0.1947782337665558\n",
      "Epoch 4 Batch 90 Loss 0.1945960372686386\n",
      "Epoch 4 Batch 99 Loss 0.19496968388557434\n",
      "Epoch 4 Batch 108 Loss 0.19459637999534607\n",
      "Epoch 4 Batch 117 Loss 0.19507315754890442\n",
      "Epoch 4 Batch 126 Loss 0.19463732838630676\n",
      "Epoch 4 Batch 135 Loss 0.19458693265914917\n",
      "Epoch 4 Batch 144 Loss 0.19467966258525848\n",
      "Epoch 4 Batch 153 Loss 0.19456352293491364\n",
      "Epoch 4 Batch 162 Loss 0.19469374418258667\n",
      "Epoch 4 Batch 171 Loss 0.19449204206466675\n",
      "Epoch 5 Batch 0 Loss 0.19455969333648682\n",
      "Epoch 5 Batch 9 Loss 0.19451045989990234\n",
      "Epoch 5 Batch 18 Loss 0.19446417689323425\n",
      "Epoch 5 Batch 27 Loss 0.19446350634098053\n",
      "Epoch 5 Batch 36 Loss 0.19432896375656128\n",
      "Epoch 5 Batch 45 Loss 0.19440701603889465\n",
      "Epoch 5 Batch 54 Loss 0.194442018866539\n",
      "Epoch 5 Batch 63 Loss 0.1943351924419403\n",
      "Epoch 5 Batch 72 Loss 0.19428499042987823\n",
      "Epoch 5 Batch 81 Loss 0.19424650073051453\n",
      "Epoch 5 Batch 90 Loss 0.19454309344291687\n",
      "Epoch 5 Batch 99 Loss 0.19431760907173157\n",
      "Epoch 5 Batch 108 Loss 0.19428128004074097\n",
      "Epoch 5 Batch 117 Loss 0.19421274960041046\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "batch_loss_frequency = train_steps_per_epoch // 18\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # Perform training steps on the training data batches\n",
    "    for (batch, (bdesc_word_ids, bdesc_input_mask, bdesc_segment_ids, bsc_ids)) in enumerate(train_data.take(train_steps_per_epoch)):\n",
    "        batch_loss = train_step(bdesc_word_ids, bdesc_input_mask, bdesc_segment_ids, bsc_ids, batch_size, n_negatives)\n",
    "        total_loss += batch_loss\n",
    "        if batch % batch_loss_frequency == 0:\n",
    "            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Redundant #####\n",
    "\n",
    "# sim_model.fit([train_desc_word_ids, train_desc_input_mask, train_desc_segment_ids, train_sc_ids], \n",
    "#               similarities, \n",
    "#               batch_size=batch_size,\n",
    "#               epochs=epochs, \n",
    "#               validation_split=0.15,\n",
    "#               verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
