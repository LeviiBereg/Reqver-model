{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import string\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tfh\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import class_weight\n",
    "from bert.tokenization import FullTokenizer\n",
    "from gensim.models import KeyedVectors as word2vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"java\" #\"python\"\n",
    "DATA_PATH = \"../../Data/code2desc\"\n",
    "DATA_FOLDER = f\"{LANGUAGE}/short\"\n",
    "TRAIN_FILE  = f\"{LANGUAGE}_train_0.jsonl\"\n",
    "TEST_FILE   = f\"{LANGUAGE}_test_0.jsonl\"\n",
    "VALID_FILE  = f\"{LANGUAGE}_valid_0.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire tokenized source code and plain docstrings.\n",
    "# BERT uses its own 'FullTokenizer' for inputs.\n",
    "use_cols = [\"code_tokens\", \"docstring\"]\n",
    "train_df = pd.read_json(f\"{DATA_PATH}/{DATA_FOLDER}/{TRAIN_FILE}\", lines=True)[use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_tokens  30000 non-null  object\n",
      " 1   docstring    30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[protected, final, void, bindIndexed, (, Confi...</td>\n",
       "      <td>Bind indexed elements to the supplied collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[public, void, setServletRegistrationBeans, (,...</td>\n",
       "      <td>Set {@link ServletRegistrationBean}s that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[public, void, addServletRegistrationBeans, (,...</td>\n",
       "      <td>Add {@link ServletRegistrationBean}s for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[public, void, setServletNames, (, Collection,...</td>\n",
       "      <td>Set servlet names that the filter will be regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[public, void, addServletNames, (, String, ......</td>\n",
       "      <td>Add servlet names for the filter.\\n@param serv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         code_tokens  \\\n",
       "0  [protected, final, void, bindIndexed, (, Confi...   \n",
       "1  [public, void, setServletRegistrationBeans, (,...   \n",
       "2  [public, void, addServletRegistrationBeans, (,...   \n",
       "3  [public, void, setServletNames, (, Collection,...   \n",
       "4  [public, void, addServletNames, (, String, ......   \n",
       "\n",
       "                                           docstring  \n",
       "0  Bind indexed elements to the supplied collecti...  \n",
       "1  Set {@link ServletRegistrationBean}s that the ...  \n",
       "2  Add {@link ServletRegistrationBean}s for the f...  \n",
       "3  Set servlet names that the filter will be regi...  \n",
       "4  Add servlet names for the filter.\\n@param serv...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at <a href=\"https://github.com/tensorflow/models/tree/master/official/nlp/bert\">tensorflow/models/official/nlp/bert</a>. It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads.\n",
    "\n",
    "This model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. For training, random input masking has been applied independently to word pieces (as in the original BERT paper).\n",
    "\n",
    "All parameters in the module are trainable, and fine-tuning all parameters is the recommended practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = tfh.KerasLayer(model_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FOLDER = \"source-code-embeddings\"\n",
    "TOKEN_EMBEDDINGS  = \"token_vecs.txt\"\n",
    "TARGET_EMBEDDINGS = \"target_vecs.txt\"\n",
    "\n",
    "vectors_text_path = f'{EMBEDDINGS_FOLDER}/{TOKEN_EMBEDDINGS}'\n",
    "model = word2vec.load_word2vec_format(vectors_text_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    '''Performs cleaning of text of unwanted symbols, \n",
    "    excessive spaces and transfers to lower-case\n",
    "    '''\n",
    "#     punct_regxp = re.compile(f'([{string.punctuation}])')\n",
    "#     text = re.sub(punct_regxp, r\" \\1 \", text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    \n",
    "    text = ''.join(character for character in text if character in string.printable)\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.docstring = train_df.docstring.apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_input(text, max_seq_length):\n",
    "\n",
    "    tokenized_text = [[\"[CLS]\"] + tokenizer.tokenize(seq)[:max_seq_length-2] + [\"[SEP]\"] for seq in text]\n",
    "    input_ids   = [tokenizer.convert_tokens_to_ids(tokens_seq) for tokens_seq in tokenized_text]\n",
    "    input_mask  = [[1] * len(input_seq) for input_seq in input_ids]\n",
    "    segment_ids = [[0] * max_seq_length for _ in range(len(input_ids))]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_seq_length, padding='post')\n",
    "    input_mask = tf.keras.preprocessing.sequence.pad_sequences(input_mask, maxlen=max_seq_length, padding='post')\n",
    "    segment_ids = tf.keras.preprocessing.sequence.pad_sequences(segment_ids, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    return input_ids, input_mask, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_max_seq_length = 256\n",
    "desc_word_ids, desc_input_mask, desc_segment_ids = generate_desc_input(train_df.docstring, desc_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sc_input(sc_inputs, emb_model, max_seq_length):\n",
    "    \n",
    "    def word_to_index(word):\n",
    "        word_val = emb_model.vocab.get(word, None)\n",
    "        word_index = word_val.index if word_val else None\n",
    "        return word_index\n",
    "    \n",
    "    input_ids = [[word_to_index(word) for word in sc_input[:max_seq_length] if word in emb_model.vocab.keys()] \\\n",
    "             for sc_input in sc_inputs]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
    "                                                        dtype='int32', \n",
    "                                                        maxlen=max_seq_length, \n",
    "                                                        padding='post')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_max_seq_length = 256\n",
    "sc_ids = generate_sc_input(train_df.code_tokens, model, sc_max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "\n",
    "# input_word_ids = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                        dtype=tf.int32,\n",
    "#                                        name=\"desc_input_word_ids\")\n",
    "# input_mask  = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                    dtype=tf.int32,\n",
    "#                                    name=\"desc_input_mask\")\n",
    "# segment_ids = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                     dtype=tf.int32,\n",
    "#                                     name=\"desc_segment_ids\")\n",
    "\n",
    "desc_dense = tf.keras.layers.Dense(dense_units, activation='sigmoid', name=\"desc_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def desc_propagate(input_word_ids, input_mask, segment_ids):\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    desc_output = desc_dense(pooled_output)\n",
    "    return desc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_get_trainable_parameters():\n",
    "    tr_vars = desc_dense.trainable_variables\n",
    "    return tr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_lstm_units = 256\n",
    "sc_model = 'convolutional' # 'lstm'\n",
    "conv_kernel_sizes = [2,3,5]\n",
    "conv_n_filters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "# input_sc_ids = tf.keras.layers.Input(shape=(sc_max_seq_length,), \n",
    "#                                        dtype=tf.int32,\n",
    "#                                        name=\"sc_input_ids\")\n",
    "\n",
    "sc_embedding = tf.keras.layers.Embedding(len(model.vocab),\n",
    "                                         model.vector_size, \n",
    "                                         weights=[model.vectors],\n",
    "                                         mask_zero=True,\n",
    "                                         trainable=False,\n",
    "                                         name=\"sc_embedding\") # (vocab_size, vec_size) (1294891, 128)\n",
    "\n",
    "if sc_model == 'convolutional':\n",
    "    sc_convs = []\n",
    "    sc_max_pools = []\n",
    "    for kernel_size in conv_kernel_sizes:\n",
    "        sc_convs.append(tf.keras.layers.Conv1D(conv_n_filters, kernel_size, activation='relu', name=f'conv_{kernel_size}'))\n",
    "        sc_max_pools.append(tf.keras.layers.MaxPooling1D(sc_max_seq_length - kernel_size + 1, 1, name=f'max_pool_{kernel_size}'))\n",
    "elif sc_model == 'lstm':\n",
    "    sc_lstm = tf.keras.layers.LSTM(sc_lstm_units, name=\"sc_lstm\")\n",
    "\n",
    "sc_dense = tf.keras.layers.Dense(dense_units, activation='sigmoid', name=\"sc_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def sc_propagate(input_sc_ids):\n",
    "    sc_embedded_input = sc_embedding(input_sc_ids) # (batch_size, sc_max_seq_length, emb_vec_size)\n",
    "    if sc_model == 'convolutional':\n",
    "        conv_outputs = []\n",
    "        for sc_conv, sc_max_pool in zip(sc_convs, sc_max_pools):\n",
    "            sc_conv_out = sc_conv(sc_embedded_input) \n",
    "            conv_outputs.append(sc_max_pool(sc_conv_out))\n",
    "        sc_output = tf.concat(conv_outputs, 2) # (batch_size, 1, n_convs * conv_n_filters)\n",
    "        sc_output = tf.reshape(sc_output, [-1, len(conv_kernel_sizes) * conv_n_filters]) # (batch_size, n_convs * conv_n_filters)\n",
    "    elif sc_model == 'lstm':\n",
    "        sc_output = sc_lstm(sc_embedded_input) #  (batch_size, sc_lstm_units)\n",
    "    sc_output = sc_dense(sc_output) # (batch_size, dense_units)\n",
    "    return sc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_get_trainable_parameters():\n",
    "    tr_vars = sc_dense.trainable_variables + sc_embedding.trainable_variables\n",
    "    if sc_model == 'convolutional':\n",
    "        for sc_conv in sc_convs:\n",
    "            tr_vars += sc_conv.trainable_variables\n",
    "    elif sc_model == 'lstm':\n",
    "        tr_vars += sc_lstm.trainable_variables\n",
    "    return tr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branches junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_mode = 'cosine' # 'cosine' 'dense'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if similarity_mode == 'dense':\n",
    "    junc_dense = tf.keras.layers.Dense(dense_units, activation='sigmoid', name='junc_dense')\n",
    "    junc_sim = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    junc_output = tf.keras.layers.Flatten()\n",
    "\n",
    "# @tf.function\n",
    "def compute_similarity(desc_output, sc_output, similarity_mode='cosine'):\n",
    "    if similarity_mode == 'cosine':\n",
    "        epsilon = 1e-10\n",
    "        norm_desc = tf.nn.l2_normalize(desc_output, axis=1, name=\"desc_output_norm\") + epsilon\n",
    "        norm_sc   = tf.nn.l2_normalize(sc_output, axis=1, name=\"sc_output_norm\") + epsilon\n",
    "        similarity = tf.reduce_sum(tf.multiply(norm_desc, norm_sc, name=\"b_outputs_dot\"), \n",
    "                                       axis=1, \n",
    "                                       name=\"cos_similarity\")\n",
    "    if similarity_mode == 'dense':\n",
    "        dense_output = junc_dense(tf.multiply(desc_output, sc_output, name=\"b_outputs_dot\"))\n",
    "        similarity = junc_sim(dense_output)\n",
    "        similarity = junc_output(similarity)\n",
    "        \n",
    "    return similarity\n",
    "\n",
    "def junc_get_trainable_parameters():\n",
    "    tr_vars = []\n",
    "    if similarity_mode == 'dense':\n",
    "        tr_vars += junc_dense.trainable_variables + junc_sim.trainable_variables\n",
    "    return tr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Redundant #####\n",
    "\n",
    "# inputs = [input_word_ids, input_mask, segment_ids, input_sc_ids]\n",
    "# outputs = cos_similarity\n",
    "\n",
    "# sim_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = tf.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = './logs/gradient_tape/' + current_time + '/train'\n",
    "val_log_dir = './logs/gradient_tape/' + current_time + '/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "# sim_model.compile(loss=loss_func, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred, n_negatives):\n",
    "\n",
    "    loss = loss_func(y_true, y_pred)\n",
    "    weight_vector = y_true * tr_weights[0] + (1.-y_true) * tr_weights[1]\n",
    "    return tf.reduce_mean(loss * weight_vector)\n",
    "\n",
    "def negative_sampling(desc_output, sc_output, n_negatives):\n",
    "    neg_probs = tf.linalg.set_diag(tf.fill([batch_size, batch_size], 0.5),[0]*batch_size)\n",
    "    neg_ids   = tf.random.categorical(neg_probs, n_negatives)\n",
    "\n",
    "    neg_desc = tf.reshape(tf.gather(desc_output, neg_ids), [-1, dense_units])\n",
    "    neg_sc   = tf.reshape(tf.gather(sc_output, [[i]*n_negatives for i in range(batch_size)]), [-1, dense_units])\n",
    "\n",
    "    desc_output = tf.concat([desc_output,neg_desc], axis=0)\n",
    "    sc_output   = tf.concat([sc_output,neg_sc], axis=0)\n",
    "    \n",
    "    return desc_output, sc_output\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_word_ids, input_mask, segment_ids, input_sc_ids, batch_size, n_negatives):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "        sc_output = sc_propagate(input_sc_ids)\n",
    "        \n",
    "        desc_output, sc_output = negative_sampling(desc_output, sc_output, n_negatives)\n",
    "        cos_similarity = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "        labels = np.array([1.] * batch_size + [0.] * (batch_size * n_negatives))\n",
    "        loss = loss_function(labels, cos_similarity, n_negatives)\n",
    "        \n",
    "    # Adjust the parameters of the model using the computed gradients\n",
    "    variables = desc_get_trainable_parameters() + sc_get_trainable_parameters() + junc_get_trainable_parameters()\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def validation_step(input_word_ids, input_mask, segment_ids, input_sc_ids, batch_size, n_negatives):\n",
    "\n",
    "    desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "    sc_output = sc_propagate(input_sc_ids)\n",
    "\n",
    "    desc_output, sc_output = negative_sampling(desc_output, sc_output, n_negatives)\n",
    "    cos_similarity = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "    labels = np.array([1.] * batch_size + [0.] * (batch_size * n_negatives))\n",
    "    loss = loss_function(labels, cos_similarity, n_negatives)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = train_test_split(desc_word_ids, desc_input_mask, desc_segment_ids, sc_ids)\n",
    "train_desc_word_ids, test_desc_word_ids = splitted_data[:2]\n",
    "train_desc_input_mask, test_desc_input_mask = splitted_data[2:4]\n",
    "train_desc_segment_ids, test_desc_segment_ids = splitted_data[4:6]\n",
    "train_sc_ids, test_sc_ids = splitted_data[6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_samples = len(train_desc_word_ids)\n",
    "valid_samples = len(test_desc_word_ids)\n",
    "train_steps_per_epoch = train_samples // batch_size\n",
    "valid_steps_per_epoch = valid_samples // batch_size\n",
    "epochs = 3\n",
    "n_negatives = 30\n",
    "val_n_negatives = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_labels = [1] * batch_size + [0] * (batch_size * n_negatives)\n",
    "val_labels = [1] * batch_size + [0] * (batch_size * val_n_negatives)\n",
    "\n",
    "# Calculate the weights for each class so that we can balance the data\n",
    "tr_weights = class_weight.compute_class_weight('balanced', [1,0], tr_labels)\n",
    "val_weights = class_weight.compute_class_weight('balanced', [1,0], val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_desc_word_ids, train_desc_input_mask, train_desc_segment_ids, train_sc_ids)).shuffle(len(train_desc_word_ids), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((test_desc_word_ids, test_desc_input_mask, test_desc_segment_ids, test_sc_ids)).shuffle(len(test_desc_word_ids), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.5081324577331543\n",
      "Epoch 1 Batch 35 Loss 0.4140643775463104\n",
      "Epoch 1 Batch 70 Loss 0.21731027960777283\n",
      "Epoch 1 Batch 105 Loss 0.17564880847930908\n",
      "Epoch 1 Batch 140 Loss 0.162144273519516\n",
      "Epoch 1 Train loss 0.3822445273399353 Val loss 0.15745973587036133 Time spent 862.0306282043457 sec\n",
      "Epoch 2 Batch 0 Loss 0.15749794244766235\n",
      "Epoch 2 Batch 35 Loss 0.15082815289497375\n",
      "Epoch 2 Batch 70 Loss 0.15110532939434052\n",
      "Epoch 2 Batch 105 Loss 0.14932052791118622\n",
      "Epoch 2 Batch 140 Loss 0.14899715781211853\n",
      "Epoch 2 Train loss 0.1511361300945282 Val loss 0.1486923098564148 Time spent 885.5952906608582 sec\n",
      "Epoch 3 Batch 0 Loss 0.14695648849010468\n",
      "Epoch 3 Batch 35 Loss 0.1483944207429886\n",
      "Epoch 3 Batch 70 Loss 0.15553124248981476\n",
      "Epoch 3 Batch 105 Loss 0.1453913450241089\n",
      "Epoch 3 Batch 140 Loss 0.15169058740139008\n",
      "Epoch 3 Train loss 0.14753691852092743 Val loss 0.14754989743232727 Time spent 884.6033086776733 sec\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "batch_loss_frequency = train_steps_per_epoch // 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss     = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # Perform training steps on the training data batches\n",
    "    for (batch, (bdesc_word_ids, bdesc_input_mask, bdesc_segment_ids, bsc_ids)) in enumerate(train_data.take(train_steps_per_epoch)):\n",
    "        batch_loss = train_step(bdesc_word_ids, bdesc_input_mask, bdesc_segment_ids, bsc_ids, batch_size, n_negatives)\n",
    "        total_loss += batch_loss\n",
    "        if batch % batch_loss_frequency == 0:\n",
    "            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy()}\")\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', total_loss, step=epoch)\n",
    "\n",
    "    for (batch, (vbdesc_word_ids, vbdesc_input_mask, vbdesc_segment_ids, vbsc_ids)) in enumerate(valid_data.take(valid_steps_per_epoch)):\n",
    "        vbatch_loss = validation_step(vbdesc_word_ids, vbdesc_input_mask, vbdesc_segment_ids, vbsc_ids, batch_size, val_n_negatives)\n",
    "        total_val_loss += vbatch_loss\n",
    "    \n",
    "    with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', total_val_loss, step=epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Train loss {total_loss/train_steps_per_epoch} Val loss {total_val_loss/valid_steps_per_epoch} Time spent {time.time()-start} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(input_word_ids, input_mask, segment_ids, input_sc_ids, n_negatives):\n",
    "    desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "    sc_output   = sc_propagate(input_sc_ids)\n",
    "\n",
    "    desc_output, sc_output = negative_sampling(desc_output, sc_output, n_negatives)    \n",
    "    cos_similarity = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3968,), dtype=float32, numpy=\n",
       "array([0.03519052, 0.02919757, 0.01732962, ..., 0.0259233 , 0.04532988,\n",
       "       0.03639392], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_word_ids, input_mask, segment_ids, input_sc_ids = list(valid_data.take(1).as_numpy_iterator())[0]\n",
    "evaluate_loss(input_word_ids, input_mask, segment_ids, input_sc_ids, val_n_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "sc_output = sc_propagate(input_sc_ids)\n",
    "\n",
    "desc_output, sc_output = negative_sampling(desc_output, sc_output, val_n_negatives)\n",
    "cos_similarity = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "labels = np.array([1.] * batch_size + [0.] * (batch_size * val_n_negatives))\n",
    "loss = loss_func(labels, cos_similarity)\n",
    "weight_vector = labels * tr_weights[0] + (1.-labels) * tr_weights[1]\n",
    "t_loss = tf.reduce_mean(weight_vector * loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3968"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3968, 256), dtype=float32, numpy=\n",
       "array([[0.07365456, 0.02421817, 0.07774226, ..., 0.00981335, 0.8692686 ,\n",
       "        0.9287045 ],\n",
       "       [0.0711057 , 0.02224155, 0.09208611, ..., 0.00774015, 0.90186703,\n",
       "        0.94541985],\n",
       "       [0.0485276 , 0.01654984, 0.06595761, ..., 0.00484712, 0.91907924,\n",
       "        0.9655349 ],\n",
       "       ...,\n",
       "       [0.06317032, 0.01878551, 0.07846779, ..., 0.00736538, 0.89793426,\n",
       "        0.9467317 ],\n",
       "       [0.06973209, 0.02633743, 0.09113064, ..., 0.00825856, 0.86922824,\n",
       "        0.9219086 ],\n",
       "       [0.0485276 , 0.01654984, 0.06595761, ..., 0.00484712, 0.91907924,\n",
       "        0.9655349 ]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3968,), dtype=float32, numpy=\n",
       "array([0.03519052, 0.02919757, 0.01732962, ..., 0.02834756, 0.03863326,\n",
       "       0.02233489], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_cos_similarity(desc_text, sc_text):\n",
    "    desc_word_ids, desc_input_mask, desc_segment_ids = generate_desc_input([desc_text], desc_max_seq_length)\n",
    "    sc_ids = generate_sc_input([sc_text], model, sc_max_seq_length)\n",
    "    \n",
    "    desc_output = desc_propagate(desc_word_ids, desc_input_mask, desc_segment_ids)\n",
    "    sc_output   = sc_propagate(sc_ids)\n",
    "\n",
    "    cos_similarity = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "    return cos_similarity.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen doc: retrieve the setter name from the field name. <p>implementation is based on the code of {@link java.beans.introspector}.</p> @param fieldname the field name @return setter name.\n",
      "Similarity: 0.028483333066105843\n",
      "Different doc: creates the ssl context for internal ssl, if internal ssl is configured. for internal ssl, the client and server side configuration are identical, because of mutual authentication.\n",
      "Similarity: 0.02656552940607071\n",
      "Inverted similarity:0.026929302141070366\n"
     ]
    }
   ],
   "source": [
    "rand_sim_ind = np.random.randint(0, len(train_df))\n",
    "rand_dif_ind = np.random.randint(0, len(train_df))\n",
    "print(f\"Chosen doc: {train_df.docstring[rand_sim_ind]}\\nSimilarity: {evaluate_cos_similarity(train_df.docstring[rand_sim_ind], train_df.code_tokens[rand_sim_ind])}\")\n",
    "print(f\"Different doc: {train_df.docstring[rand_dif_ind]}\\nSimilarity: {evaluate_cos_similarity(train_df.docstring[rand_sim_ind], train_df.code_tokens[rand_dif_ind])}\")\n",
    "print(f\"Inverted similarity:{evaluate_cos_similarity(train_df.docstring[rand_dif_ind], train_df.code_tokens[rand_sim_ind])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
