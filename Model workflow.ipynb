{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import string\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tfh\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import class_weight\n",
    "from bert.tokenization import FullTokenizer\n",
    "from gensim.models import KeyedVectors as word2vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"java\" #\"python\"\n",
    "DATA_PATH = \"../../Data/code2desc\"\n",
    "DATA_FOLDER = f\"{LANGUAGE}/short\"\n",
    "TRAIN_FILE  = f\"{LANGUAGE}_train_0.jsonl\"\n",
    "TEST_FILE   = f\"{LANGUAGE}_test_0.jsonl\"\n",
    "VALID_FILE  = f\"{LANGUAGE}_valid_0.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire tokenized source code and plain docstrings.\n",
    "# BERT uses its own 'FullTokenizer' for inputs.\n",
    "use_cols = [\"code_tokens\", \"docstring\"]\n",
    "train_df = pd.read_json(f\"{DATA_PATH}/{DATA_FOLDER}/{TRAIN_FILE}\", lines=True)[use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_tokens  30000 non-null  object\n",
      " 1   docstring    30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[protected, final, void, bindIndexed, (, Confi...</td>\n",
       "      <td>Bind indexed elements to the supplied collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[public, void, setServletRegistrationBeans, (,...</td>\n",
       "      <td>Set {@link ServletRegistrationBean}s that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[public, void, addServletRegistrationBeans, (,...</td>\n",
       "      <td>Add {@link ServletRegistrationBean}s for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[public, void, setServletNames, (, Collection,...</td>\n",
       "      <td>Set servlet names that the filter will be regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[public, void, addServletNames, (, String, ......</td>\n",
       "      <td>Add servlet names for the filter.\\n@param serv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         code_tokens  \\\n",
       "0  [protected, final, void, bindIndexed, (, Confi...   \n",
       "1  [public, void, setServletRegistrationBeans, (,...   \n",
       "2  [public, void, addServletRegistrationBeans, (,...   \n",
       "3  [public, void, setServletNames, (, Collection,...   \n",
       "4  [public, void, addServletNames, (, String, ......   \n",
       "\n",
       "                                           docstring  \n",
       "0  Bind indexed elements to the supplied collecti...  \n",
       "1  Set {@link ServletRegistrationBean}s that the ...  \n",
       "2  Add {@link ServletRegistrationBean}s for the f...  \n",
       "3  Set servlet names that the filter will be regi...  \n",
       "4  Add servlet names for the filter.\\n@param serv...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at <a href=\"https://github.com/tensorflow/models/tree/master/official/nlp/bert\">tensorflow/models/official/nlp/bert</a>. It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads.\n",
    "\n",
    "This model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. For training, random input masking has been applied independently to word pieces (as in the original BERT paper).\n",
    "\n",
    "All parameters in the module are trainable, and fine-tuning all parameters is the recommended practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = tfh.KerasLayer(model_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FOLDER = \"source-code-embeddings\"\n",
    "TOKEN_EMBEDDINGS  = \"token_vecs.txt\"\n",
    "TARGET_EMBEDDINGS = \"target_vecs.txt\"\n",
    "\n",
    "vectors_text_path = f'{EMBEDDINGS_FOLDER}/{TOKEN_EMBEDDINGS}'\n",
    "model = word2vec.load_word2vec_format(vectors_text_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    '''Performs cleaning of text of unwanted symbols, \n",
    "    excessive spaces and transfers to lower-case\n",
    "    '''\n",
    "#     punct_regxp = re.compile(f'([{string.punctuation}])')\n",
    "#     text = re.sub(punct_regxp, r\" \\1 \", text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    \n",
    "    text = ''.join(character for character in text if character in string.printable)\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.docstring = train_df.docstring.apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_input(text, max_seq_length):\n",
    "\n",
    "    tokenized_text = [[\"[CLS]\"] + tokenizer.tokenize(seq)[:max_seq_length-2] + [\"[SEP]\"] for seq in text]\n",
    "    input_ids   = [tokenizer.convert_tokens_to_ids(tokens_seq) for tokens_seq in tokenized_text]\n",
    "    input_mask  = [[1] * len(input_seq) for input_seq in input_ids]\n",
    "    segment_ids = [[0] * max_seq_length for _ in range(len(input_ids))]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_seq_length, padding='post')\n",
    "    input_mask = tf.keras.preprocessing.sequence.pad_sequences(input_mask, maxlen=max_seq_length, padding='post')\n",
    "    segment_ids = tf.keras.preprocessing.sequence.pad_sequences(segment_ids, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    return input_ids, input_mask, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_max_seq_length = 256\n",
    "desc_word_ids, desc_input_mask, desc_segment_ids = generate_desc_input(train_df.docstring, desc_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sc_input(sc_inputs, emb_model, max_seq_length):\n",
    "    \n",
    "    def word_to_index(word):\n",
    "        word_val = emb_model.vocab.get(word, None)\n",
    "        word_index = word_val.index if word_val else None\n",
    "        return word_index\n",
    "    \n",
    "    input_ids = [[word_to_index(word) for word in sc_input[:max_seq_length] if word in emb_model.vocab.keys()] \\\n",
    "             for sc_input in sc_inputs]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
    "                                                        dtype='int32', \n",
    "                                                        maxlen=max_seq_length, \n",
    "                                                        padding='post')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_max_seq_length = 256\n",
    "sc_ids = generate_sc_input(train_df.code_tokens, model, sc_max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "\n",
    "# input_word_ids = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                        dtype=tf.int32,\n",
    "#                                        name=\"desc_input_word_ids\")\n",
    "# input_mask  = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                    dtype=tf.int32,\n",
    "#                                    name=\"desc_input_mask\")\n",
    "# segment_ids = tf.keras.layers.Input(shape=(desc_max_seq_length,), \n",
    "#                                     dtype=tf.int32,\n",
    "#                                     name=\"desc_segment_ids\")\n",
    "\n",
    "desc_dense = tf.keras.layers.Dense(dense_units, activation='sigmoid', name=\"desc_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def desc_propagate(input_word_ids, input_mask, segment_ids):\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    desc_output = desc_dense(pooled_output)\n",
    "    return desc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_get_trainable_parameters():\n",
    "    tr_vars = desc_dense.trainable_variables\n",
    "    return tr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_lstm_units = 256\n",
    "sc_model = 'convolutional' # 'lstm'\n",
    "conv_kernel_sizes = [2,3,5]\n",
    "conv_n_filters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Migrated into `train_step` function #####\n",
    "\n",
    "# input_sc_ids = tf.keras.layers.Input(shape=(sc_max_seq_length,), \n",
    "#                                        dtype=tf.int32,\n",
    "#                                        name=\"sc_input_ids\")\n",
    "\n",
    "sc_embedding = tf.keras.layers.Embedding(len(model.vocab),\n",
    "                                         model.vector_size, \n",
    "                                         weights=[model.vectors],\n",
    "                                         mask_zero=True,\n",
    "                                         trainable=False,\n",
    "                                         name=\"sc_embedding\") # (vocab_size, vec_size) (1294891, 128)\n",
    "\n",
    "if sc_model == 'convolutional':\n",
    "    sc_convs = []\n",
    "    sc_max_pools = []\n",
    "    for kernel_size in conv_kernel_sizes:\n",
    "        sc_convs.append(tf.keras.layers.Conv1D(conv_n_filters, kernel_size, activation='relu', name=f'conv_{kernel_size}'))\n",
    "        sc_max_pools.append(tf.keras.layers.MaxPooling1D(sc_max_seq_length - kernel_size + 1, 1, name=f'max_pool_{kernel_size}'))\n",
    "elif sc_model == 'lstm':\n",
    "    sc_lstm = tf.keras.layers.LSTM(sc_lstm_units, name=\"sc_lstm\")\n",
    "\n",
    "sc_dense = tf.keras.layers.Dense(dense_units, activation='sigmoid', name=\"sc_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def sc_propagate(input_sc_ids):\n",
    "    sc_embedded_input = sc_embedding(input_sc_ids) # (batch_size, sc_max_seq_length, emb_vec_size)\n",
    "    if sc_model == 'convolutional':\n",
    "        conv_outputs = []\n",
    "        for sc_conv, sc_max_pool in zip(sc_convs, sc_max_pools):\n",
    "            sc_conv_out = sc_conv(sc_embedded_input) \n",
    "            conv_outputs.append(sc_max_pool(sc_conv_out))\n",
    "        sc_output = tf.concat(conv_outputs, 2) # (batch_size, 1, n_convs * conv_n_filters)\n",
    "        sc_output = tf.reshape(sc_output, [-1, len(conv_kernel_sizes) * conv_n_filters]) # (batch_size, n_convs * conv_n_filters)\n",
    "    elif sc_model == 'lstm':\n",
    "        sc_output = sc_lstm(sc_embedded_input) #  (batch_size, sc_lstm_units)\n",
    "    sc_output = sc_dense(sc_output) # (batch_size, dense_units)\n",
    "    return sc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_get_trainable_parameters():\n",
    "    tr_vars = sc_dense.trainable_variables + sc_embedding.trainable_variables\n",
    "    if sc_model == 'convolutional':\n",
    "        for sc_conv in sc_convs:\n",
    "            tr_vars += sc_conv.trainable_variables\n",
    "    elif sc_model == 'lstm':\n",
    "        tr_vars += sc_lstm.trainable_variables\n",
    "    return tr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branches junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_mode = 'cosine' # 'cosine' 'dense'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if similarity_mode == 'dense':\n",
    "    junc_dense = tf.keras.layers.Dense(dense_units, activation='sigmoid', name='junc_dense')\n",
    "    junc_sim = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    junc_output = tf.keras.layers.Flatten()\n",
    "\n",
    "# @tf.function\n",
    "def compute_similarity(desc_output, sc_output, similarity_mode='cosine'):\n",
    "    if similarity_mode == 'cosine':\n",
    "#         norm_desc = tf.nn.l2_normalize(desc_output, axis=1, name=\"desc_output_norm\")\n",
    "#         norm_sc   = tf.nn.l2_normalize(sc_output, axis=1, name=\"sc_output_norm\") + epsilon\n",
    "#         similarity = tf.reduce_sum(tf.multiply(norm_desc, norm_sc, name=\"b_outputs_dot\"), \n",
    "#                                        axis=1, \n",
    "#                                        name=\"cos_similarity\")\n",
    "\n",
    "        norm_desc = tf.norm(desc_output, axis=-1, keepdims=True) + 1e-10\n",
    "        norm_sc = tf.norm(sc_output, axis=-1, keepdims=True) + 1e-10\n",
    "        cosine_similarities = tf.matmul(desc_output / norm_desc,\n",
    "                                        sc_output / norm_sc,\n",
    "                                        transpose_a=False,\n",
    "                                        transpose_b=True,\n",
    "                                        name='code_query_cooccurrence_logits')  # (batch_size, batch_size)\n",
    "        similarity_scores = cosine_similarities\n",
    "\n",
    "        # A max-margin-like loss, but do not penalize negative cosine similarities.\n",
    "        neg_matrix = tf.linalg.diag(tf.fill(dims=[tf.shape(cosine_similarities)[0]], value=float('-inf')))\n",
    "        per_sample_loss = tf.maximum(0., loss_margin\n",
    "                                         - tf.linalg.diag_part(cosine_similarities)\n",
    "                                         + tf.reduce_max(tf.nn.relu(cosine_similarities + neg_matrix),\n",
    "                                                         axis=-1))\n",
    "\n",
    "        per_sample_loss = per_sample_loss * labels\n",
    "        similarity = tf.reduce_sum(per_sample_loss) / tf.reduce_sum(labels)\n",
    "\n",
    "#         # extract the logits from the diagonal of the matrix, which are the logits corresponding to the ground-truth\n",
    "#         correct_scores = tf.linalg.diag_part(similarity_scores)\n",
    "#         # compute how many queries have bigger logits than the ground truth (the diagonal) -> which will be incorrectly ranked\n",
    "#         compared_scores = similarity_scores >= tf.expand_dims(correct_scores, axis=-1)\n",
    "#         # for each row of the matrix (query), sum how many logits are larger than the ground truth\n",
    "#         # ...then take the reciprocal of that to get the MRR for each individual query (you will need to take the mean later)\n",
    "#         mrr = 1 / tf.reduce_sum(tf.cast(compared_scores, dtype=tf.float32), axis=1)\n",
    "\n",
    "    if similarity_mode == 'dense':\n",
    "        dense_output = junc_dense(tf.multiply(desc_output, sc_output, name=\"b_outputs_dot\"))\n",
    "        similarity = junc_sim(dense_output)\n",
    "        similarity = junc_output(similarity)\n",
    "        \n",
    "    return similarity\n",
    "\n",
    "def junc_get_trainable_parameters():\n",
    "    tr_vars = []\n",
    "    if similarity_mode == 'dense':\n",
    "        tr_vars += junc_dense.trainable_variables + junc_sim.trainable_variables\n",
    "    return tr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Redundant #####\n",
    "\n",
    "# inputs = [input_word_ids, input_mask, segment_ids, input_sc_ids]\n",
    "# outputs = cos_similarity\n",
    "\n",
    "# sim_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = tf.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = './logs/gradient_tape/' + current_time + '/train'\n",
    "val_log_dir = './logs/gradient_tape/' + current_time + '/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "# sim_model.compile(loss=loss_func, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred, n_negatives):\n",
    "\n",
    "    loss = loss_func(y_true, y_pred)\n",
    "#     weight_vector = y_true * tr_weights[0] + (1.-y_true) * tr_weights[1]\n",
    "    return tf.reduce_mean(loss)# * weight_vector)\n",
    "\n",
    "def negative_sampling(desc_output, sc_output, n_negatives):\n",
    "    neg_probs = tf.linalg.set_diag(tf.fill([batch_size, batch_size], 0.5),[0]*batch_size)\n",
    "    neg_ids   = tf.random.categorical(neg_probs, n_negatives)\n",
    "\n",
    "    neg_desc = tf.reshape(tf.gather(desc_output, neg_ids), [-1, dense_units])\n",
    "    neg_sc   = tf.reshape(tf.gather(sc_output, [[i]*n_negatives for i in range(batch_size)]), [-1, dense_units])\n",
    "\n",
    "    desc_output = tf.concat([desc_output,neg_desc], axis=0)\n",
    "    sc_output   = tf.concat([sc_output,neg_sc], axis=0)\n",
    "    \n",
    "    return desc_output, sc_output\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_word_ids, input_mask, segment_ids, input_sc_ids, batch_size, n_negatives):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "        sc_output = sc_propagate(input_sc_ids)\n",
    "        \n",
    "#         desc_output, sc_output = negative_sampling(desc_output, sc_output, n_negatives)\n",
    "        loss = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "#         labels = np.array([1.] * batch_size + [0.] * (batch_size * n_negatives))\n",
    "#         loss = loss_function(labels, cos_similarity, n_negatives)\n",
    "        \n",
    "    # Adjust the parameters of the model using the computed gradients\n",
    "    variables = desc_get_trainable_parameters() + sc_get_trainable_parameters() + junc_get_trainable_parameters()\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def validation_step(input_word_ids, input_mask, segment_ids, input_sc_ids, batch_size, n_negatives):\n",
    "\n",
    "    desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "    sc_output = sc_propagate(input_sc_ids)\n",
    "\n",
    "#     desc_output, sc_output = negative_sampling(desc_output, sc_output, n_negatives)\n",
    "    loss = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "#     labels = np.array([1.] * batch_size + [0.] * (batch_size * n_negatives))\n",
    "#     loss = loss_function(labels, cos_similarity, n_negatives)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data = train_test_split(desc_word_ids, desc_input_mask, desc_segment_ids, sc_ids)\n",
    "train_desc_word_ids, test_desc_word_ids = splitted_data[:2]\n",
    "train_desc_input_mask, test_desc_input_mask = splitted_data[2:4]\n",
    "train_desc_segment_ids, test_desc_segment_ids = splitted_data[4:6]\n",
    "train_sc_ids, test_sc_ids = splitted_data[6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_samples = len(train_desc_word_ids)\n",
    "valid_samples = len(test_desc_word_ids)\n",
    "train_steps_per_epoch = train_samples // batch_size\n",
    "valid_steps_per_epoch = valid_samples // batch_size\n",
    "epochs = 3\n",
    "n_negatives = 30\n",
    "val_n_negatives = 30\n",
    "loss_margin = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_labels  = [1] * batch_size + [0] * (batch_size * n_negatives)\n",
    "# val_labels = [1] * batch_size + [0] * (batch_size * val_n_negatives)\n",
    "\n",
    "# # Calculate the weights for each class so that we can balance the data\n",
    "# tr_weights  = class_weight.compute_class_weight('balanced', [1,0], tr_labels)\n",
    "# val_weights = class_weight.compute_class_weight('balanced', [1,0], val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_desc_word_ids, train_desc_input_mask, train_desc_segment_ids, train_sc_ids)).shuffle(len(train_desc_word_ids), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((test_desc_word_ids, test_desc_input_mask, test_desc_segment_ids, test_sc_ids)).shuffle(len(test_desc_word_ids), reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.compat.v1.placeholder_with_default(input=np.ones(shape=[batch_size],dtype=np.float32),\n",
    "                                        shape=[batch_size],\n",
    "                                        name='sample_loss_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.0117988586425781\n",
      "Epoch 1 Batch 17 Loss 1.0000001192092896\n",
      "Epoch 1 Batch 34 Loss 1.0\n",
      "Epoch 1 Batch 51 Loss 1.0\n",
      "Epoch 1 Batch 68 Loss 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-dddaea255098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Perform training steps on the training data batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbdesc_word_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbdesc_input_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbdesc_segment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsc_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbdesc_word_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbdesc_input_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbdesc_segment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsc_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_negatives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_loss_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Science\\Python\\Programs\\ProjectModel\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Science\\Python\\Programs\\ProjectModel\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Science\\Python\\Programs\\ProjectModel\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Science\\Python\\Programs\\ProjectModel\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Science\\Python\\Programs\\ProjectModel\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Science\\Python\\Programs\\ProjectModel\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Science\\Python\\Programs\\ProjectModel\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "batch_loss_frequency = train_steps_per_epoch // 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss     = 0.0\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # Perform training steps on the training data batches\n",
    "    for (batch, (bdesc_word_ids, bdesc_input_mask, bdesc_segment_ids, bsc_ids)) in enumerate(train_data.take(train_steps_per_epoch)):\n",
    "        batch_loss = train_step(bdesc_word_ids, bdesc_input_mask, bdesc_segment_ids, bsc_ids, batch_size, n_negatives)\n",
    "        total_loss += batch_loss\n",
    "        if batch % batch_loss_frequency == 0:\n",
    "            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy()}\")\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', total_loss, step=epoch)\n",
    "\n",
    "    for (batch, (vbdesc_word_ids, vbdesc_input_mask, vbdesc_segment_ids, vbsc_ids)) in enumerate(valid_data.take(valid_steps_per_epoch)):\n",
    "        vbatch_loss = validation_step(vbdesc_word_ids, vbdesc_input_mask, vbdesc_segment_ids, vbsc_ids, batch_size, val_n_negatives)\n",
    "        total_val_loss += vbatch_loss\n",
    "    \n",
    "    with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', total_val_loss, step=epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Train loss {total_loss/train_steps_per_epoch} Val loss {total_val_loss/valid_steps_per_epoch} Time spent {time.time()-start} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(input_word_ids, input_mask, segment_ids, input_sc_ids, n_negatives):\n",
    "    desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "    sc_output   = sc_propagate(input_sc_ids)\n",
    "\n",
    "    cos_similarity = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_word_ids, input_mask, segment_ids, input_sc_ids = list(valid_data.take(1).as_numpy_iterator())[0]\n",
    "evaluate_loss(input_word_ids, input_mask, segment_ids, input_sc_ids, val_n_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "sc_output = sc_propagate(input_sc_ids)\n",
    "\n",
    "norm_desc = tf.norm(desc_output, axis=-1, keepdims=True) + 1e-10\n",
    "norm_sc = tf.norm(sc_output, axis=-1, keepdims=True) + 1e-10\n",
    "cosine_similarities = tf.matmul(desc_output / norm_desc,\n",
    "                                sc_output / norm_sc,\n",
    "                                transpose_a=False,\n",
    "                                transpose_b=True,\n",
    "                                name='code_query_cooccurrence_logits')  # (batch_size, batch_size)\n",
    "similarity_scores = cosine_similarities\n",
    "\n",
    "# A max-margin-like loss, but do not penalize negative cosine similarities.\n",
    "neg_matrix = tf.linalg.diag(tf.fill(dims=[tf.shape(cosine_similarities)[0]], value=float('-inf')))\n",
    "per_sample_loss = tf.maximum(0., loss_margin\n",
    "                                 - tf.linalg.diag_part(cosine_similarities)\n",
    "                                 + tf.reduce_max(tf.nn.relu(cosine_similarities + neg_matrix),\n",
    "                                                 axis=-1))\n",
    "\n",
    "per_sample_loss = per_sample_loss * labels\n",
    "similarity = tf.reduce_sum(per_sample_loss) / tf.reduce_sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the logits from the diagonal of the matrix, which are the logits corresponding to the ground-truth\n",
    "correct_scores = tf.linalg.diag_part(similarity_scores)\n",
    "# compute how many queries have bigger logits than the ground truth (the diagonal) -> which will be incorrectly ranked\n",
    "compared_scores = similarity_scores >= tf.expand_dims(correct_scores, axis=-1)\n",
    "# for each row of the matrix (query), sum how many logits are larger than the ground truth\n",
    "# ...then take the reciprocal of that to get the MRR for each individual query (you will need to take the mean later)\n",
    "mrr = 1 / tf.reduce_sum(tf.cast(compared_scores, dtype=tf.float32), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 256), dtype=float32, numpy=\n",
       "array([[0.7653763 , 0.7653763 , 0.7653763 , ..., 0.7653763 , 0.7653763 ,\n",
       "        0.7653763 ],\n",
       "       [0.7729839 , 0.7729839 , 0.7729839 , ..., 0.7729839 , 0.7729839 ,\n",
       "        0.7729839 ],\n",
       "       [0.76702464, 0.76702464, 0.76702464, ..., 0.76702464, 0.76702464,\n",
       "        0.76702464],\n",
       "       ...,\n",
       "       [0.77066565, 0.77066565, 0.77066565, ..., 0.77066565, 0.77066565,\n",
       "        0.77066565],\n",
       "       [0.7738836 , 0.7738836 , 0.7738836 , ..., 0.7738836 , 0.7738836 ,\n",
       "        0.7738836 ],\n",
       "       [0.76643455, 0.76643455, 0.76643455, ..., 0.76643455, 0.76643455,\n",
       "        0.76643455]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(per_sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_margin = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.linalg.diag_part(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_output = desc_propagate(input_word_ids, input_mask, segment_ids)\n",
    "sc_output = sc_propagate(input_sc_ids)\n",
    "\n",
    "norm_desc = tf.norm(desc_output, axis=-1, keepdims=True) + 1e-10\n",
    "norm_sc = tf.norm(sc_output, axis=-1, keepdims=True) + 1e-10\n",
    "cosine_similarities = tf.matmul(desc_output / norm_desc,\n",
    "                                sc_output / norm_sc,\n",
    "                                transpose_a=False,\n",
    "                                transpose_b=True,\n",
    "                                name='code_query_cooccurrence_logits',\n",
    "                                )  # B x B\n",
    "similarity_scores = cosine_similarities\n",
    "\n",
    "# A max-margin-like loss, but do not penalize negative cosine similarities.\n",
    "neg_matrix = tf.linalg.diag(tf.fill(dims=[tf.shape(cosine_similarities)[0]], value=float('-inf')))\n",
    "per_sample_loss = tf.maximum(0., loss_margin\n",
    "                                 - tf.linalg.diag_part(cosine_similarities)\n",
    "                                 + tf.reduce_max(tf.nn.relu(cosine_similarities + neg_matrix),\n",
    "                                                 axis=-1))\n",
    "\n",
    "per_sample_loss = per_sample_loss * labels\n",
    "loss = tf.reduce_sum(per_sample_loss) / tf.reduce_sum(labels)\n",
    "\n",
    "# extract the logits from the diagonal of the matrix, which are the logits corresponding to the ground-truth\n",
    "correct_scores = tf.linalg.diag_part(similarity_scores)\n",
    "# compute how many queries have bigger logits than the ground truth (the diagonal) -> which will be incorrectly ranked\n",
    "compared_scores = similarity_scores >= tf.expand_dims(correct_scores, axis=-1)\n",
    "# for each row of the matrix (query), sum how many logits are larger than the ground truth\n",
    "# ...then take the reciprocal of that to get the MRR for each individual query (you will need to take the mean later)\n",
    "mrr = 1 / tf.reduce_sum(tf.cast(compared_scores, dtype=tf.float32), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_cos_similarity(desc_text, sc_text):\n",
    "    desc_word_ids, desc_input_mask, desc_segment_ids = generate_desc_input([desc_text], desc_max_seq_length)\n",
    "    sc_ids = generate_sc_input([sc_text], model, sc_max_seq_length)\n",
    "    \n",
    "    desc_output = desc_propagate(desc_word_ids, desc_input_mask, desc_segment_ids)\n",
    "    sc_output   = sc_propagate(sc_ids)\n",
    "\n",
    "    cos_similarity = compute_similarity(desc_output, sc_output, similarity_mode)\n",
    "    return cos_similarity.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sim_ind = np.random.randint(0, len(train_df))\n",
    "rand_dif_ind = np.random.randint(0, len(train_df))\n",
    "print(f\"Chosen doc: {train_df.docstring[rand_sim_ind]}\\nSimilarity: {evaluate_cos_similarity(train_df.docstring[rand_sim_ind], train_df.code_tokens[rand_sim_ind])}\")\n",
    "print(f\"Different doc: {train_df.docstring[rand_dif_ind]}\\nSimilarity: {evaluate_cos_similarity(train_df.docstring[rand_sim_ind], train_df.code_tokens[rand_dif_ind])}\")\n",
    "print(f\"Inverted similarity:{evaluate_cos_similarity(train_df.docstring[rand_dif_ind], train_df.code_tokens[rand_sim_ind])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
