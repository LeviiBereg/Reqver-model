{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as tfh\n",
    "from bert.tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"java\" #\"python\"\n",
    "DATA_PATH = \"../../Data/code2desc\"\n",
    "DATA_FOLDER = f\"{LANGUAGE}/short\"\n",
    "TRAIN_FILE  = f\"{LANGUAGE}_train_0.jsonl\"\n",
    "TEST_FILE   = f\"{LANGUAGE}_test_0.jsonl\"\n",
    "VALID_FILE  = f\"{LANGUAGE}_valid_0.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire tokenized source code and plain docstrings.\n",
    "# BERT uses its own 'FullTokenizer' for inputs.\n",
    "use_cols = [\"code_tokens\", \"docstring\"]\n",
    "train_df = pd.read_json(f\"{DATA_PATH}/{DATA_FOLDER}/{TRAIN_FILE}\", lines=True)[use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_tokens  30000 non-null  object\n",
      " 1   docstring    30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at <a href=\"https://github.com/tensorflow/models/tree/master/official/nlp/bert\">tensorflow/models/official/nlp/bert</a>. It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads.\n",
    "\n",
    "This model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs have been \"uncased\", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. For training, random input masking has been applied independently to word pieces (as in the original BERT paper).\n",
    "\n",
    "All parameters in the module are trainable, and fine-tuning all parameters is the recommended practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = tfh.KerasLayer(model_url,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    '''Performs cleaning of text of unwanted symbols, \n",
    "    excessive spaces and transfers to lower-case\n",
    "    '''\n",
    "#     punct_regxp = re.compile(f'([{string.punctuation}])')\n",
    "#     text = re.sub(punct_regxp, r\" \\1 \", text)\n",
    "    text = re.sub(r'\\s+', \" \", text)\n",
    "    \n",
    "    text = ''.join(character for character in text if character in string.printable)\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.docstring = train_df.docstring.apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_input(text, max_seq_length):\n",
    "\n",
    "    tokenized_text = [[\"[CLS]\"] + tokenizer.tokenize(seq)[:max_seq_length-2] + [\"[SEP]\"] for seq in text]\n",
    "    input_ids   = [tokenizer.convert_tokens_to_ids(tokens_seq) for tokens_seq in tokenized_text]\n",
    "    input_mask  = [[1] * len(input_seq) for input_seq in input_ids]\n",
    "    segment_ids = [[0] * max_seq_length for _ in range(len(input_ids))]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_seq_length, padding='post')\n",
    "    input_mask = tf.keras.preprocessing.sequence.pad_sequences(input_mask, maxlen=max_seq_length, padding='post')\n",
    "    segment_ids = tf.keras.preprocessing.sequence.pad_sequences(segment_ids, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    return input_ids, input_mask, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "train_word_ids, train_input_mask, train_segment_ids = generate_bert_input(train_df.docstring, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), \n",
    "                                       dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), \n",
    "                                   dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), \n",
    "                                    dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "\n",
    "desc_dense = tf.keras.layers.Dense(dense_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "desc_output = desc_dense(pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointdir = \"java14_model\"\n",
    "chkpoint_prefix = os.path.join(checkpointdir, \"saved_model_iter8.release\")\n",
    "if not os.path.exists(checkpointdir):\n",
    "    os.mkdir(checkpointdir)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
    "                                 decoderNetwork = decoderNetwork)\n",
    "\n",
    "try:\n",
    "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpointdir))\n",
    "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(checkpointdir)))\n",
    "except:\n",
    "    print(\"No checkpoint found at {}\".format(checkpointdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branches junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [input_word_ids, input_mask, segment_ids]\n",
    "outputs = []\n",
    "\n",
    "sim_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "loss_func = tf.keras.losses.CosineSimilarity()\n",
    "\n",
    "sim_model.compile(loss=loss_func, \n",
    "                   optimizer=optimizer, \n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
